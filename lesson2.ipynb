{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Systems in Agriculture (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Understanding basics of PyTorch\n",
    "\n",
    "PyTorch is a machine/deep learning framework based on the Torch library. It was developed by Meta AI and has broad application in computer vision and natural language processing.\n",
    "\n",
    "[PyTorch website](https://pytorch.org/)\n",
    "\n",
    "[60-minute tutorial on Deep Learning with PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html): Please go over this in your spare time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Image object detection using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to take:\n",
    "1. Find an existing performant model trained on dataset same as your target dataset\n",
    "2. Initialize the model and pretrained model weights\n",
    "3. Preprocess the image to fit the input requirements of the model\n",
    "4. Make the prediction\n",
    "5. Analyze the prediction results\n",
    "6. Visualize the prediction results\n",
    "7. Evaluate the results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Install PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# In your python virtual environment, run the commands\n",
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find an existing performant model trained on dataset same as your target dataset\n",
    "\n",
    "Looking over the [PyTorch model list](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights), we select the [MaskRCNN model with the ResNet-50-FCN](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialize the model and pretrained model weights\n",
    "\n",
    "We are using [COCO dataset](https://cocodataset.org/#explore): Microsoft Common Objects in Context dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "# 1. obtain and process weights\n",
    "weights = MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "transforms = weights.transforms()\n",
    "\n",
    "# 2. initialize the model\n",
    "model = maskrcnn_resnet50_fpn(weights=weights, progress=False)\n",
    "model = model.eval()\n",
    "\n",
    "# 3. explore the model details\n",
    "# print(model)\n",
    "\n",
    "# 4. TODO: Get the categories (class labels) from the metadata. See if you can identify all the fruit/vegetables categories\n",
    "# categories = weights.meta[\"categories\"]\n",
    "# print(categories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import target image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. load the image \n",
    "image_path = '.\\img\\orange.jpg'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# 2. display the image\n",
    "# plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preprocess the image to fit the input requirements of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "# convert to a tensor and normalizes the data as well as shape of the image\n",
    "preprocess = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "img_preprocess = preprocess(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "# TODO: Explore the shape/size of both input image and the preprocessed version\n",
    "# print(f'Shape of original image: {image.size}')\n",
    "# print(f'Shape of original image: {img_preprocess.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Perform model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "with torch.no_grad():\n",
    "    predictions = model(img_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Analyze prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract boxes, labels, scores, and masks from predictions\n",
    "boxes = predictions[0]['boxes'].cpu().detach().numpy()\n",
    "labels = predictions[0]['labels'].cpu().detach().numpy()\n",
    "scores = predictions[0]['scores'].cpu().detach().numpy()\n",
    "masks = predictions[0]['masks'].cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# TODO: Explore the prediction results. Notice the prediction scores\n",
    "# for i in range(len(scores)):\n",
    "#     print(f'Prediction {i+1}')\n",
    "#     print('--------------------------')\n",
    "#     print(f'Label: {categories[labels[i]]}')\n",
    "#     print(f'Score: {scores[i]}')\n",
    "#     print(f'Bbox: {boxes[i]}')\n",
    "#     print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore the masks for each prediction\n",
    "# score_threshold = 0.5\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# for i in range(len(masks)):\n",
    "#     if scores[i] > score_threshold:\n",
    "#         plt.subplot(1, len(masks),i+1)\n",
    "#         plt.imshow(masks[i][0], cmap='gray')\n",
    "#         plt.title(f'Mask for prediction {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Import the image using cv.imread \n",
    "\n",
    "\n",
    "score_threshold = 0.6\n",
    "\n",
    "# TODO: Using the threshold to filter predictions, apply the equivalent mask and draw the bounding box (using cv.rectangle()) on the image\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Plot the original image and the segmentation/bounding box image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_ground_truth_ann, calculate_map\n",
    "\n",
    "# Our goal here is to evaluate the segmentation and object detection prediction against ground truth data for the image\n",
    "\n",
    "# TODO: Extract the bounding box information from the prediction result above and place into the prediction_bbox variable. Define a score threshold to use\n",
    "\n",
    "prediction_bbox = []\n",
    "\n",
    "\n",
    "\n",
    "# Here we have provided a helper function to extract the ground truth bbox for the images\n",
    "# image_name = 'orange'\n",
    "# ground_truth_bbox = get_ground_truth_ann(image_name=image_name, show=False)\n",
    "\n",
    "# print(f'ground_truths = {ground_truth_bbox}')\n",
    "# print(f'predictions = {prediction_bbox}')\n",
    "\n",
    "\n",
    "# TODO: plot the results\n",
    "# img_bbox = img.copy()\n",
    "\n",
    "# for gt in ground_truth_bbox:\n",
    "#     gt = [int(v) for v in gt]\n",
    "#     cv.rectangle(img_bbox, (gt[0], gt[1]), (gt[2], gt[3]), (0, 255, 0), 2)\n",
    "\n",
    "# for pd in prediction_bbox:\n",
    "#     pd = [int(v) for v in pd]\n",
    "#     cv.rectangle(img_bbox, (pd[0], pd[1]), (pd[2], pd[3]), (255, 0, 0), 2)\n",
    "\n",
    "\n",
    "# plt.imshow(img_bbox)\n",
    "# plt.title(\"Ground truth vs predicted Bbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the calculate_map helper function to calculate the average precision of your object detection pipeline\n",
    "# mAP = calculate_map(ground_truth_bbox, prediction_bbox, iou_threshold=0.7)\n",
    "# print(mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional exploration\n",
    "\n",
    "Now you have walked though the process using the images we analyzed in the last class, we will run through the pipeline using another image with multiple fruit/vegetables.\n",
    "\n",
    "Your task is to get an image online with fruits (e.g., apple, banana, orange) and run it through the object detection model and visualize the result.\n",
    "\n",
    "One thing to do is to adjust the color of the visualization to match different fruit.\n",
    "\n",
    "See example solution below.\n",
    "\n",
    "<img src=\".\\img\\solution3.png\" width=\"800\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
